{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer</th>\n",
       "      <th>Question</th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>SupportingFacts</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>Were Scott Derrickson and Ed Wood of the same ...</td>\n",
       "      <td>5a8b57f25542995d1e6f1371</td>\n",
       "      <td>[[Scott Derrickson, 0], [Ed Wood, 0]]</td>\n",
       "      <td>[Adam Collis is an American filmmaker and acto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chief of Protocol</td>\n",
       "      <td>What government position was held by the woman...</td>\n",
       "      <td>5a8c7595554299585d9e36b6</td>\n",
       "      <td>[[Kiss and Tell (1945 film), 0], [Shirley Temp...</td>\n",
       "      <td>[A Kiss for Corliss is a 1949 American comedy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Animorphs</td>\n",
       "      <td>What science fantasy young adult series, told ...</td>\n",
       "      <td>5a85ea095542994775f606a8</td>\n",
       "      <td>[[The Hork-Bajir Chronicles, 0], [The Hork-Baj...</td>\n",
       "      <td>[Animorphs is a science fantasy series of youn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>Are the Laleli Mosque and Esma Sultan Mansion ...</td>\n",
       "      <td>5adbf0a255429947ff17385a</td>\n",
       "      <td>[[Laleli Mosque, 0], [Esma Sultan Mansion, 0]]</td>\n",
       "      <td>[Esma Sultan is the name of three daughters of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Greenwich Village, New York City</td>\n",
       "      <td>The director of the romantic comedy \"Big Stone...</td>\n",
       "      <td>5a8e3ea95542995a26add48d</td>\n",
       "      <td>[[Big Stone Gap (film), 0], [Adriana Trigiani,...</td>\n",
       "      <td>[Great Eastern Conventions, Inc. was an entert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97847</th>\n",
       "      <td>American</td>\n",
       "      <td>Kerry Remsen is the daughter of an actor with ...</td>\n",
       "      <td>5a8f8db25542997ba9cb32b9</td>\n",
       "      <td>[[Kerry Remsen, 1], [Bert Remsen, 0]]</td>\n",
       "      <td>[Kerry Remsen is an American actress.,  She is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97848</th>\n",
       "      <td>Simon Property Group</td>\n",
       "      <td>Who manages both Northshore Mall in Peabody, M...</td>\n",
       "      <td>5ae4f3615542993aec5ec0fd</td>\n",
       "      <td>[[Northshore Mall, 0], [Northshore Mall, 4], [...</td>\n",
       "      <td>[Green Tree Mall is a shopping mall located in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97849</th>\n",
       "      <td>Amblin Partners</td>\n",
       "      <td>Charlee Johnson was part of a band that signed...</td>\n",
       "      <td>5a903fc95542990a984935bd</td>\n",
       "      <td>[[Charlee Johnson, 4], [DreamWorks, 0]]</td>\n",
       "      <td>[Simon M. Woods is a British entrepreneur and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97850</th>\n",
       "      <td>MV \"Wilhelm Gustloff</td>\n",
       "      <td>What is the ship that sank in the Baltic sea a...</td>\n",
       "      <td>5ab56e71554299494045efc8</td>\n",
       "      <td>[[Salt to the Sea, 1], [MV Wilhelm Gustloff, 0]]</td>\n",
       "      <td>[The I.V. Stalin White Sea – Baltic Sea Canal ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97851</th>\n",
       "      <td>2017 Southeast Asian Games</td>\n",
       "      <td>Vietnam national cricket team will debut at wh...</td>\n",
       "      <td>5ac28e915542996366519a0a</td>\n",
       "      <td>[[Vietnam national cricket team, 0], [Vietnam ...</td>\n",
       "      <td>[The Vietnam national cricket team represents ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97852 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Answer  \\\n",
       "0                                   yes   \n",
       "1                     Chief of Protocol   \n",
       "2                             Animorphs   \n",
       "3                                    no   \n",
       "4      Greenwich Village, New York City   \n",
       "...                                 ...   \n",
       "97847                          American   \n",
       "97848              Simon Property Group   \n",
       "97849                   Amblin Partners   \n",
       "97850              MV \"Wilhelm Gustloff   \n",
       "97851        2017 Southeast Asian Games   \n",
       "\n",
       "                                                Question  \\\n",
       "0      Were Scott Derrickson and Ed Wood of the same ...   \n",
       "1      What government position was held by the woman...   \n",
       "2      What science fantasy young adult series, told ...   \n",
       "3      Are the Laleli Mosque and Esma Sultan Mansion ...   \n",
       "4      The director of the romantic comedy \"Big Stone...   \n",
       "...                                                  ...   \n",
       "97847  Kerry Remsen is the daughter of an actor with ...   \n",
       "97848  Who manages both Northshore Mall in Peabody, M...   \n",
       "97849  Charlee Johnson was part of a band that signed...   \n",
       "97850  What is the ship that sank in the Baltic sea a...   \n",
       "97851  Vietnam national cricket team will debut at wh...   \n",
       "\n",
       "                     QuestionId  \\\n",
       "0      5a8b57f25542995d1e6f1371   \n",
       "1      5a8c7595554299585d9e36b6   \n",
       "2      5a85ea095542994775f606a8   \n",
       "3      5adbf0a255429947ff17385a   \n",
       "4      5a8e3ea95542995a26add48d   \n",
       "...                         ...   \n",
       "97847  5a8f8db25542997ba9cb32b9   \n",
       "97848  5ae4f3615542993aec5ec0fd   \n",
       "97849  5a903fc95542990a984935bd   \n",
       "97850  5ab56e71554299494045efc8   \n",
       "97851  5ac28e915542996366519a0a   \n",
       "\n",
       "                                         SupportingFacts  \\\n",
       "0                  [[Scott Derrickson, 0], [Ed Wood, 0]]   \n",
       "1      [[Kiss and Tell (1945 film), 0], [Shirley Temp...   \n",
       "2      [[The Hork-Bajir Chronicles, 0], [The Hork-Baj...   \n",
       "3         [[Laleli Mosque, 0], [Esma Sultan Mansion, 0]]   \n",
       "4      [[Big Stone Gap (film), 0], [Adriana Trigiani,...   \n",
       "...                                                  ...   \n",
       "97847              [[Kerry Remsen, 1], [Bert Remsen, 0]]   \n",
       "97848  [[Northshore Mall, 0], [Northshore Mall, 4], [...   \n",
       "97849            [[Charlee Johnson, 4], [DreamWorks, 0]]   \n",
       "97850   [[Salt to the Sea, 1], [MV Wilhelm Gustloff, 0]]   \n",
       "97851  [[Vietnam national cricket team, 0], [Vietnam ...   \n",
       "\n",
       "                                                 Context  \n",
       "0      [Adam Collis is an American filmmaker and acto...  \n",
       "1      [A Kiss for Corliss is a 1949 American comedy ...  \n",
       "2      [Animorphs is a science fantasy series of youn...  \n",
       "3      [Esma Sultan is the name of three daughters of...  \n",
       "4      [Great Eastern Conventions, Inc. was an entert...  \n",
       "...                                                  ...  \n",
       "97847  [Kerry Remsen is an American actress.,  She is...  \n",
       "97848  [Green Tree Mall is a shopping mall located in...  \n",
       "97849  [Simon M. Woods is a British entrepreneur and ...  \n",
       "97850  [The I.V. Stalin White Sea – Baltic Sea Canal ...  \n",
       "97851  [The Vietnam national cricket team represents ...  \n",
       "\n",
       "[97852 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "DATA_DIR = '../data/hotpotqa-fullwiki/'\n",
    "file_names = ['hotpot_dev_fullwiki_v1.json', 'hotpot_train_v1.1.json']\n",
    "data_list = []\n",
    "\n",
    "def count_words(sentences):\n",
    "    return sum(len(sentence.split()) for sentence in sentences)\n",
    "\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(DATA_DIR, file_name)\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "        for item in json_data:\n",
    "            answer = item[\"answer\"]\n",
    "            question = item[\"question\"]\n",
    "            question_id = item[\"_id\"]\n",
    "            \n",
    "            supporting_facts = item[\"supporting_facts\"]\n",
    "            context_list = []\n",
    "            for context in item[\"context\"]:\n",
    "                context_list.extend(context[1])\n",
    "                \n",
    "            data_list.append({\n",
    "                \"Answer\": answer,\n",
    "                \"Question\": question,\n",
    "                \"QuestionId\": question_id,\n",
    "                \"SupportingFacts\": supporting_facts,\n",
    "                \"Context\": context_list\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions: 97852\n",
      "Average question length: 17.66 words\n",
      "Standard deviation of question length: 9.29 words\n",
      "Number of contexts (facts): 4017967\n",
      "Average context length: 888.86 words\n",
      "Standard deviation of context length: 252.84 words\n",
      "Average golden answer length: 2.24 words\n",
      "Standard deviation of golden answer length: 1.81 words\n"
     ]
    }
   ],
   "source": [
    "num_questions = df.shape[0]\n",
    "\n",
    "df['QuestionLength'] = df['Question'].apply(lambda x: len(x.split()))\n",
    "avg_question_length = df['QuestionLength'].mean()\n",
    "std_question_length = df['QuestionLength'].std()\n",
    "\n",
    "df['NumContexts'] = df['Context'].apply(lambda x: len(x))\n",
    "num_contexts = df['NumContexts'].sum()\n",
    "\n",
    "df['ContextLength'] = df['Context'].apply(lambda x: count_words(x))\n",
    "avg_context_length = df['ContextLength'].mean()\n",
    "std_context_length = df['ContextLength'].std()\n",
    "\n",
    "df['AnswerLength'] = df['Answer'].apply(lambda x: len(x.split()))\n",
    "avg_answer_length = df['AnswerLength'].mean()\n",
    "std_answer_length = df['AnswerLength'].std()\n",
    "\n",
    "print(f\"Number of questions: {num_questions}\")\n",
    "print(f\"Average question length: {avg_question_length:.2f} words\")\n",
    "print(f\"Standard deviation of question length: {std_question_length:.2f} words\")\n",
    "print(f\"Number of contexts (facts): {num_contexts}\")\n",
    "print(f\"Average context length: {avg_context_length:.2f} words\")\n",
    "print(f\"Standard deviation of context length: {std_context_length:.2f} words\")\n",
    "print(f\"Average golden answer length: {avg_answer_length:.2f} words\")\n",
    "print(f\"Standard deviation of golden answer length: {std_answer_length:.2f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/yuehengzhang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b9658a78d94db2bf04a17f6dba37ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing n-grams:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_top_ngrams(questions, n, top_k=None):\n",
    "    ngram_counter = Counter()\n",
    "    for question in questions:\n",
    "        tokens = nltk.word_tokenize(question.lower())\n",
    "        ngram_counter.update(ngrams(tokens, n))\n",
    "    total = sum(ngram_counter.values())\n",
    "    if not top_k:\n",
    "        top_k = len(ngram_counter)\n",
    "    return ngram_counter.most_common(top_k), total\n",
    "\n",
    "def process_ngrams(questions, n_values, top_k=None):\n",
    "    df_ngram = pd.DataFrame()\n",
    "    for n in tqdm(n_values, desc=\"Processing n-grams\"):\n",
    "        top_ngrams, total = get_top_ngrams(questions, n, top_k)\n",
    "        ngram_data = [(n, rank + 1, ' '.join(ngram), freq, freq / total) for rank, (ngram, freq) in enumerate(top_ngrams)]\n",
    "        df_temp = pd.DataFrame(ngram_data, columns=['n', 'rank', 'ngram', 'frequency', 'relative_frequency'])\n",
    "        df_ngram = pd.concat([df_ngram, df_temp], ignore_index=True)\n",
    "    return df_ngram\n",
    "\n",
    "questions = df['Question'].tolist()\n",
    "\n",
    "if os.path.isfile(os.path.join(DATA_DIR, 'ngrams-sorted.csv')):\n",
    "    df_ngram = pd.read_csv(os.path.join(DATA_DIR, 'ngrams-sorted.csv'))\n",
    "else: \n",
    "    df_ngram = process_ngrams(questions, range(1, 9))\n",
    "    df_ngram.to_csv(os.path.join(DATA_DIR, 'ngrams-sorted.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HotpotQA n-gram Ratios:\n",
      "               1         2         4         8        16        32        64       128       256    Top 1%  Total Count\n",
      "1-gram  0.061074  0.110454  0.175492  0.267616  0.368172  0.446158  0.508694  0.571567  0.634060  0.742319        85455\n",
      "2-gram  0.011113  0.017692  0.028908  0.043062  0.059473  0.082975  0.116189  0.155650  0.199684  0.445840       523428\n",
      "3-gram  0.002774  0.004643  0.007121  0.011354  0.017336  0.025421  0.035185  0.048569  0.065058  0.237728      1018623\n",
      "4-gram  0.001226  0.002201  0.004111  0.006555  0.008834  0.012189  0.016696  0.022321  0.029666  0.127703      1294261\n",
      "5-gram  0.001020  0.001784  0.002578  0.003459  0.004729  0.006417  0.008575  0.011326  0.015188  0.072521      1380837\n",
      "6-gram  0.000808  0.001016  0.001390  0.001904  0.002495  0.003254  0.004389  0.005946  0.008028  0.044717      1368190\n",
      "7-gram  0.000077  0.000128  0.000226  0.000411  0.000715  0.001179  0.001784  0.002630  0.003848  0.030164      1309324\n",
      "8-gram  0.000042  0.000075  0.000139  0.000247  0.000407  0.000653  0.001022  0.001535  0.002286  0.024391      1229365\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "def calculate_ratios(df_ngram, n_values, k_values):\n",
    "    ratio_sums = {k: [] for k in k_values}\n",
    "    top_1_percent_ratios = []\n",
    "    total_counts = []\n",
    "    \n",
    "    for n in n_values:\n",
    "        subset = df_ngram[df_ngram['n'] == n]\n",
    "        ratios = subset['relative_frequency'].tolist()\n",
    "        total_count = len(ratios)\n",
    "        total_counts.append(total_count)\n",
    "        sorted_ratios = sorted(ratios, reverse=True)\n",
    "        \n",
    "        top_1_percent_count = max(1, total_count // 100)\n",
    "        top_1_percent_ratios.append(sum(sorted_ratios[:top_1_percent_count]))\n",
    "        \n",
    "        for k in k_values:\n",
    "            top_k_ratios = sorted_ratios[:k]\n",
    "            ratio_sums[k].append(sum(top_k_ratios))\n",
    "    \n",
    "    return ratio_sums, top_1_percent_ratios, total_counts\n",
    "\n",
    "n_values = list(range(1, 9))\n",
    "k_values = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "\n",
    "ratios, top_1_percent_ratios, total_counts = calculate_ratios(df_ngram, n_values, k_values)\n",
    "\n",
    "ratios['Top 1%'] = top_1_percent_ratios\n",
    "ratios['Total Count'] = total_counts\n",
    "\n",
    "print(\"HotpotQA n-gram Ratios:\")\n",
    "print(pd.DataFrame(ratios, index=[f'{n}-gram' for n in n_values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
